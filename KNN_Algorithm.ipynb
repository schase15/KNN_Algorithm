{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN_Algorithm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNC8Xl1qPg5ihAq0oYG5h09",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schase15/KNN_Algorithm/blob/master/KNN_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6CiSLUhhlLj",
        "colab_type": "text"
      },
      "source": [
        "# Implementing K Nearest Neighbors By Hand"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZyTDkUchlOo",
        "colab_type": "text"
      },
      "source": [
        "### This notebook includes the code for implementing the K Nearest Neighbors algorithm by hand\n",
        "My algorithm for K Nearest Neighbors is implemented as a Python class using only base python, numpy and scipy.\n",
        "\n",
        "I then selected a data set and compare the results of my own KNN algorithm to the results of the standard KNN from sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grqIZnvtjyX3",
        "colab_type": "text"
      },
      "source": [
        "## Theory Behind KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jP41SdNj2F-",
        "colab_type": "text"
      },
      "source": [
        "K Nearest Neighbors is a simple machine learning model that makes predictions based off of the most similar observations in its traing data. While it can be very powerful, its predictive capability is limited to observations that are similar to what the training data it has in memory.\n",
        "\n",
        "Unlike most other models, KNN does not 'learn' from its training dataset. Instead it holds the entire training set in memory and then compares the new observation to its stored data. KNN performs no work until a prediction is required.\n",
        "\n",
        "When a prediction is required it does exactly what it name says. The model examines the new observation and finds the most similar records (nearest neighbors) that it holds in its training set. The number of neighbors (k) the model selects from its training data is defined by the user. \n",
        "\n",
        "A prediction can be made by either returning the most common outcome (classification) or by taking the average (regression).\n",
        "\n",
        "**Some Important Notes on Using KNN**\n",
        "\n",
        "- KNN is a simple model to implement, but as a result it is limited in the types of data it can take as input. When working with KNN the phrase \"garbage in, garbage out\" is never more accurate. KNN does not handle categorical variables so everything must be pre-processed to include numerical values only. Additionally, as you will see in the next section, the nearest neighbors are found by calculating the distances between the new observation and the records held in memory. Those with the smallest distances are considered most similar. Intuatively, you should understand the importance of scaling your data (so that they are all being measured on the same metric) before running a KNN model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRn_G7faj2Nl",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm Implementation\n",
        "Now that we understand the theory behind KNN, we can implement our own algorithm from scratch in three setps.\n",
        "\n",
        "Step 1. Calculate Euclidean Distance\n",
        "\n",
        "Step 2. Get Nearest Neighbors\n",
        "\n",
        "Step 3. Make Predicitons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFH3IvvGj2MM",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Calculate Euclidean Distance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9lBTiIU9ReO",
        "colab_type": "text"
      },
      "source": [
        "The formula for Euclidean Distance is:\n",
        "\n",
        "$ \\sum_{i=1}^n (x_{i}-y_{i})^2 $\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYxuyLnBw6vh",
        "colab_type": "text"
      },
      "source": [
        "The Euclidean Distance may sound complicated, and the formula may look intimitating. But the concept is very simple. The Euclidean Distance is the ordinary straight line distance between two data points. The formula can be simply derived from the Pythagorean formula: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLubqoBkw6zF",
        "colab_type": "text"
      },
      "source": [
        "Pythagorean Theorem:\n",
        "\n",
        "$c^2 = a^2 + b^2 $\n",
        "\n",
        "Where c is the Euclidean distance between datapoints a and b.\n",
        "\n",
        "For simplicity, let's first say that data point a and b are 2-Dimensial and described by their x and y coordinates.\n",
        "\n",
        "a= ($a_{1}, a_{2}$) and b= ($b_{1}, b_{2}$).\n",
        "\n",
        "To help with understanding, visually we can view this on a graph. On the graph below data points a and b have been ploted (represented by the large arrowheads). The Euclidean distance we are trying to calculate is the vector drawn in yellow.\n",
        "\n",
        "By drawing in the vectors representing the datapoints (in blue and red) we can clearly see that the yellow Euclidean distance is simply the hypotenuse of the triangle. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKQHZH14m0q2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "0a293094-24d6-4416-be27-056fad3f509b"
      },
      "source": [
        "# Import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Coordinate Pairs for where the datapoints are (represented by the arrowheads \n",
        "# at the end of the vectors)\n",
        "\n",
        "# Datapoint a\n",
        "red = (2, 0)\n",
        "\n",
        "# Datapoint b\n",
        "blue = (0, 2)\n",
        "\n",
        "# Axis Bounds\n",
        "plt.xlim(-1,3)          \n",
        "plt.ylim(-1,3)\n",
        "\n",
        "# Plot Vectors\n",
        "plt.arrow(0, 2, 2, -2, head_width=.02, head_length=0.01, color = 'y')\n",
        "plt.arrow(0, 0, 2, 0, head_width=.2, head_length=0.1, color = 'r')\n",
        "plt.arrow(0, 0, 0, 2,  head_width=.2, head_length=0.1, color = 'b')\n",
        "plt.title('Calculating Euclidean Distance')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcdZ3u8c/Tnc5OQkISskASAmEJgRBoQkKWbgecARwJOqi4IAjeiMuIiuM4ei9XHcdt3MaBGWQABeUCIqKBgVFEu5NA9pBAFiALQhIC2cgesvX3/lG/aKfpTndS1X2qu5/361WvPnXOr8751qmueupsv1JEYGZmVpJ1AWZmVhwcCGZmBjgQzMwscSCYmRngQDAzs8SBYGZmgAPBEkkh6ZQ85/EVST/P4/FLJFXmU0NLklQl6aNp+IOSfteUtsVG0uOSrsm6DsueA6ENkfQBSfMk7ZC0Lr3RJ2RdV30k/VTS12uPi4gzI6KqGZZ1raQDab3Uvg0s1DIi4t6I+OtCza9QUtDvTM93k6QnJb2vdpuIuDQi7m7ivPL60mDFzYHQRkj6HPBD4BvA8cBg4D+AyVnWVURmRkT3OrdXsy6qhYyKiO7AacBPgVsk/d9sS7Ji5EBoAyT1BL4GfDIifhUROyNiX0Q8EhH/kNqMkTRT0pa09XCLpI4NzK+LpO9JelnSVkkz0rhKSWvqtP2TpIsbmM+Dkl5L85gm6cw0fgrwQeAL6ZvrI3XnlXY//ULSPZK2p91J5bXmfa6kZ9K0ByU9UHeL4wjW3yHffOtuvUiaLGmhpG2SVkq6pJ55XCtpRq37b5f0fHrutwCq0/46ScskvSHpt5KG1Jr2b5JWp+XNlzSx1rTDrpfDiYiNEfEz4OPAP0k6Ls2z9q6vUyRVp7o3SnogjZ+WZrMovWbvk9RL0qOSNqTn8aikE2rVWiXpnyU9lWr9naQ+taZPkPR0+p9cLenaNL6TpO9KekXS65Juk9SlKc/R8uNAaBvGAZ2Bhw/T5gDwWaBPan8R8IkG2n4XOA+4EOgNfAGoOYq6HgeGA/2ABcC9ABFxexr+Tvqm/s4GHn85cD9wLDAVuAUgBdnD5L7t9gbuA951FPU1StIY4B7gH1Idk4A/NfKYPsCvgP9Nbn2vBMbXmj4Z+BLwbqAvMJ3cczhoLnAOuef2/4AHJXWuNb3e9XIEfgN0AMbUM+2fgd8BvYATgH8HiIhJafqo9Jo9QO7z4yfAEHJbpLvrqeUDwEfI/Q90BD4PkALw8TT/vun5LkyP+RZwahp3CjAIuPkIn6MdBQdC23AcsDEi9jfUICLmR8SsiNgfEX8CfgxU1G0nqQS4DrgxItZGxIGIeDoi9hxpURFxV0RsT4/9CjAqbc001YyIeCwiDgA/A0al8WPJfaD9KG0J/QqY08i8xqZvogdvK5tYw/XAXRHxRETUpHXyfCOPuQxYEhG/jIh95HblvVZr+g3ANyNiWXrNvgGcc3ArISJ+HhGb0mv1PaATud09BzW0Xpok1bSRXODUtY/cB/zAiHgzImbU0+bgfDZFxEMRsSsitgP/wlv/p34SES9GxG7gF+Q+5CEXFL+PiPvSa7gpIhZKEjAF+GxEbE7z/QZw1ZE8Rzs6DoS2YRPQR1KHhhpIOjVt0r8maRu5N1mfepr2Ibe10dQPzIaWVyrpW2kXyzb+8q26vmU2pPaH6C6gc3qOA4G1cWjPjKsbmdesiDi21u3kJtZwIke+LgbWrifVWbu+IcC/HQwnYDO5XUqDACR9Pu1O2pqm9+TQ9dbQemkSSWXkvpVvrmfyF1Itc9LuqOsOM5+ukn6s3K7FbcA04FhJpYeptXsabmi99gW6AvNrrZ//SeOtmTkQ2oaZwB7gisO0+U/geWB4RPQgt8tC9bTbCLwJ1PeBuZPcmxXIfejT8Bv1A+QOaF9M7gNt6MGHpb/5dLO7DhiUvk0edGIe89tFrecF9K81vJr618XhrKtdT6qzdn2rgY/VCaguEfF0Ol7wBeC9QK+IOBbYSv2v1dGaDOynnq2qiHgtIv5XRAwEPgb8hxo+s+gmclsuF6T/qYO7lZpSa0PrdSO5XU9n1lo3PdNBcWtmDoQ2ICK2ktvHequkK9I3tzJJl0r6Tmp2DLAN2CHpdHIHFuubVw1wF/B9SQPTN/1xkjoBL5L7NvqO9C3zf5PbnVGfY8iF1CZyH7bfqDP9dWDYUT7lmeSOiXxKUoe0T76+/eFNtRD4QHqul3Dobo87gY9IukhSiaRBaf0dzn8DZ0p6d/rm/mkODZnbyB3UPXiQvaek96Rpx5D7sN4AdJB0M9Ajj+f2Z5J6S/ogcCvw7YjYVE+b99Q6MPwGueA+ePyo7mt2DLkP7y2SegNHcubSvcDFkt6bXsPjJJ2T/v/+C/iBpH6ppkGS/uYI5m1HyYHQRqR9zZ8j9yG9gdw3sE8Bv05NPk/uW/t2cm+4Bw4zu88Dz5E7uLkZ+DZQkoLnE8AdwFpyWwxrGpjHPcDLqd1SYFad6XcCI9JugV/XffDhRMRecgdkrwe2AB8CHiUXQA0Zp7deh3B+mnYj8M40rw/yl3VGRMwhd1D0B+S+qVeT2+VzuPo2Au8hd3B0E7kD60/Vmv4wuXV6f9rVshi4NE3+LbldJC+SW39v0vjusMYskrQDWAF8lNz++YYO0p4PzE7tp5I7lrQqTfsKcHd6zd5L7thIF3Lf6melupskIl4hd6zlJnL/Ywv5y7GQf0y1zkrr5/ccegzFmon8AznWFkiaDdwWET/Juhaz1spbCNYqSaqQ1D/tbrgGOJsj+IZqZm+VdyBI6ixpjqRF6ayEr9bTppNyFw6tkDRb0tB8l2vt3mnAInK7eW4CroyIddmWZNa65b3LKJ1B0S0idqQDjTPI7XecVavNJ4CzI+IGSVcB74qI9zUwSzMzy0DeWwiRsyPdLUu3uikzGTjYedYvgYvqnDJoZmYZa/LFLIeTzkefT+4y81sjYnadJoNIZ0pExH5JW0lX19aZzxRyVynSrVu3804/vbGz+8zMrLb58+dvjIijupCvIIGQLqE/R9KxwMOSRkbE4qOYz+3A7QDl5eUxb968QpRnZtZuSHr5aB9b0LOMImIL8Eegbm+Qa0lXaqYLdXqSOz/bzMyKRCHOMuqbtgxQrovat5PrIqG2qcDBX2S6EvhD+AIIM7OiUohdRgPIXb1YSi5gfhERj0r6GjAvIqaSuyr1Z5JWkLsq0T0XmpkVmbwDISKeBUbXM/7mWsNvkruU38zMipSvVDYzM8CBYGZmiQPBzMwAB4KZmSUOBDMzAxwIZmaWOBDMzAxwIJiZWeJAMDMzwIFgZmaJA8HMzAAHgpmZJQ4EMzMDHAhmZpY4EMzMDHAgmJlZ4kAwMzPAgWBmZknegSDpREl/lLRU0hJJN9bTplLSVkkL0+3m+uZlZmbZyfs3lYH9wE0RsUDSMcB8SU9ExNI67aZHxN8WYHlmZtYM8t5CiIh1EbEgDW8HlgGD8p2vmZm1rIIeQ5A0FBgNzK5n8jhJiyQ9LunMQi7XzMzyV4hdRgBI6g48BHwmIrbVmbwAGBIROyRdBvwaGF7PPKYAUwAGDx5cqNLMzKwJCrKFIKmMXBjcGxG/qjs9IrZFxI40/BhQJqlPPe1uj4jyiCjv27dvIUozM7MmKsRZRgLuBJZFxPcbaNM/tUPSmLTcTfku28zMCqcQu4zGA1cDz0lamMZ9CRgMEBG3AVcCH5e0H9gNXBURUYBlm5lZgeQdCBExA1AjbW4Bbsl3WWZm1nx8pbKZmQEOBDMzSxwIZmYGOBDMzCxxIJiZGeBAMDOzxIFgZmaAA8HMzBIHgpmZAQ4EMzNLHAhmZgY4EMzMLHEgmJkZ4EAwM7PEgWBmZoADoV14803Y5N+nM7NGOBDauKoqGDYMTjgBvvtdOHAg64rMrFg5ENqorVvhmmvgHe+AdetyWwlf+QqMHg1Ll2ZdnZkVIwdCG/TII7mtggcegF27/jJ+505YvBjKy+Hmm2Hv3uxqNLPik3cgSDpR0h8lLZW0RNKN9bSRpB9JWiHpWUnn5rtce6sNG+CKK+Cqq2DzZtiz561tImD3bvje9+CMM2Du3Jav08yKUyG2EPYDN0XECGAs8ElJI+q0uRQYnm5TgP8swHItiYB774VTToHHHjt0q6Ahu3bBqlVQUQE33ti0x5hZ29Yh3xlExDpgXRreLmkZMAiovad6MnBPRAQwS9Kxkgakx1qefvaz3PGCsjLo2vWt03ftCvbtEz16HEAqPWTavn3wox/BK6/Aww+3UMFmVpTyDoTaJA0FRgOz60waBKyudX9NGndIIEiaQm4LgsGDBxeytDZt8mSYObPh6dddFyxbJr75zfH06HEhJ5/8PSQd0mbAgGYu0syKXsECQVJ34CHgMxGx7WjmERG3A7cDlJeXR6Fqa+t69oSxYxuefvLJJSxbBpMnX83y5Z9iz54fMHr00/TsOa7lijSzoleQs4wklZELg3sj4lf1NFkLnFjr/glpnLWgQYM+yYQJ24ESnnnmQubOPYcIX5hgZjmFOMtIwJ3Asoj4fgPNpgIfTmcbjQW2+vhBNjp06E5l5QFGjLifnTsXUV3dgU2bHs+6LDMrAoXYZTQeuBp4TtLCNO5LwGCAiLgNeAy4DFgB7AI+UoDlWh769XsfffpcwezZw3nuucsoK+vH2LEvU1raOevSzCwjhTjLaAagRtoE8Ml8l2WFVVLSiXHjXmHz5id49tm/Zvr0Lpx++s/o3/9DWZdmZhnwlcpG795vp6JiP8ccM4bnn7+aqiqxf/9RnRdgZq2YA8EAkEo577zZnHvuHABmzOjJ6tU/zLgqM2tJDgQ7RI8e51NRUUOfPlewcuVnqaoSe/euz7osM2sBDgR7C0mMHPkw55+/DICnnz6elSu/mHFVZtbcHAjWoG7dTqeyMhg48BOsXv1tqqrE7t0vZV2WmTUTB4I16tRTb2Xs2FzPI7NnD2PZsmvInThmZm2JA8GapHPnE6isDIYO/Rqvv34P1dUl7NjxXNZlmVkBORDsiAwd+n8YPz73A83z5p3NokVvJ6Im46rMrBAcCHbEysp6U1kZnHrqbbzxxu+pri5ly5YZWZdlZnlyINhRGzjwY0ycuIOSks4sXDiROXPOpKZmf9ZlmdlRciBYXkpLuzFp0m5GjHiQXbuWMm1aGRs3PpJ1WWZ2FBwIVhD9+l3JpEl76Nz5ZBYvvpwZM3pz4MDurMsysyPgQLCCKSnpyNixKxg16kn273+D6dO7sm7dT7Muy8yayIFgBder119RUXGAHj0m8MILH0md5W3Nuiwza4QDwZqFVMK5507nvPPmATBjxrG88sq/ZlyVmR2OA8Ga1THHnEdFRQ19+76HVau+QFWV2LPntazLMrN6OBCs2UnizDN/wZgxLwIwc+YAVqy4KeOqzKwuB4K1mK5dh1NZGQwadCNr1nyfqiqxa9eKrMsys6QggSDpLknrJS1uYHqlpK2SFqbbzYVYrrVOw4f/kHHj1gIwZ85wli79gDvLMysChdpC+ClwSSNtpkfEOen2tQIt11qpTp0GUlkZnHTSN1m//j6qq0vYvn1h1mWZtWsFCYSImAZsLsS8rH0ZMuSLjB+f+9eZP380zzxT6c7yzDLSkscQxklaJOlxSWfW10DSFEnzJM3bsGFDC5ZmWSor60VlZXDaaXewdWt16iyvOuuyzNqdlgqEBcCQiBgF/Dvw6/oaRcTtEVEeEeV9+/ZtodKsWAwYcD0TJ+6ktLQHCxdWMnv2qdTU7Mu6LLN2o0UCISK2RcSONPwYUCapT0ss21qX0tKuTJy4lZEjf83u3cuZNq0jGzY8nHVZZu1CiwSCpP6SlIbHpOVuaollW+vUp89kJk3aS5cup7FkybuZNq07Bw7syrosszatUKed3gfMBE6TtEbS9ZJukHRDanIlsFjSIuBHwFXh8wytESUlZVxwwfOcc04VNTU7mT69G6++ekfWZZm1WSrWz+Xy8vKYN29e1mW0Ce98Jzz6KBTpS90kETUsWvR2tmz5AwDjx2+mrKxXxlWZFR9J8yOi/Gge6yuVrVWQSjjnnCcpL89dq/DUU715+eVvZlyVWdviQLBWpXv3UVRU1NCv3wd56aUvpc7yXs26LLM2wYFgrY4kRoz4ORdcsBKAmTMHsXz5pzOuyqz1cyBYq9WlyzAqK4MTTriJtWv/PXWW92LWZZm1Wg4Ea/VOOeW7jBu3DoA5c05jyZL3uLM8s6PgQLA2oVOn/lRWBsOG/SsbNvwydZY3P+uyzFoVB4K1KYMHf54JE7YAMH9+OQsWjHdneWZN5ECwNqdDh55UVgann34327Y9TXV1KW+88WTWZZkVPQeCtVn9+3+YiRN306HDcSxadDEzZw6lpmZv1mWZFS0HgrVppaWdmTBhIyNHPsKePS8zbVon1q9/MOuyzIqSA8HahT59/pZJk/bRrdtZLF36XqqrO3LgwM6syzIrKg4EazdKSjpw/vnPMnr0DCL2MX16d9auvS3rssyKhgPB2p2ePcdTUVFDr15/w/LlH6eqSuzb597YzRwI1i5JYtSo/6G8/DkAnnqqDy+99NWMqzLLlgPB2rXu3UdSUVFD//7X8vLLX6GqSrz55uqsyzLLhAPB2j1JnH76T7jggpcAmDVrMC+++HF3f2HtjgPBLOnSZSiVlcHgwf/Eq6/eRnV1CTt3Pp91WWYtxoFgVsewYd/gwgtfB2Du3DN47rkrvLVg7UKhflP5LknrJS1uYLok/UjSCknPSjq3EMs1ay4dO/ajsjI45ZR/Y9Om31BdXcK2bXOyLsusWRVqC+GnwCWHmX4pMDzdpgD/WaDlmjWrE074NBMmbANgwYILmD//fCIOZFyVWfMoSCBExDRg82GaTAbuiZxZwLGSBhRi2WbNrUOHY6isDM444162b59HdXUHNm/+XdZlmRVcSx1DGATUPpdvTRp3CElTJM2TNG/Dhg0tVJpZ0xx//AeYNOlNOnYcwLPP/g1PPz2Impo9WZdlVjBFdVA5Im6PiPKIKO/bt2/W5Zi9RUlJJy688FXOOutx9u59lWnTOvP66/dnXZZZQbRUIKwFTqx1/4Q0zqxVOu64S6io2E/37uexbNn7qaoS+/dvz7oss7y0VCBMBT6czjYaC2yNiHUttGyzZiGVUl4+j9GjZwIwY0YP1qy5JeOqzI5eoU47vQ+YCZwmaY2k6yXdIOmG1OQxYBWwAvgv4BOFWK5ZMejZcywVFTUcd9zfsmLF31NVJfbu9TEwa306FGImEfH+RqYH8MlCLMusGEnirLMeYefOpcydeyZPP92PwYO/zLBhX8+6NLMmK6qDymatXbduI6ioqGHAgCm88sq/pM7yXs66LLMmcSCYFZgkTjvtx4wdmwuCWbOG8vzz17v7Cyt6DgSzZtK582AqK4MhQ27mtdfuSp3lLcm6LLMGORDMmtlJJ32VCy/MHWSeO3ckzz57mbcWrCg5EMxaQMeOfaisDIYPv5XNmx+nurqErVufzross0M4EMxa0KBBn2DChO1IZTzzzHjmzh1FTc3+rMsyAxwIZi2uQ4fuVFTsZcSIB9i581mmTStj06bHsi7LzIFglpV+/d7LpEl76NRpCM899w6eeqofBw68mXVZ1o45EMwyVFLSkXHj/sTZZz/Bvn0bmD69C6+99vOsy7J2yoFgVgR6976Yior99Ogxluefvzp1lrct67KsnXEgmBUJqZRzz53JuefOBWDGjJ6sXv2DjKuy9sSBYFZkevQop6Kihj593s3KlZ9LneW9nnVZ1g44EMyKkCRGjnyIMWOeB+Dpp/uzcuU/ZlyVtXUOBLMi1rXraVRWBgMHfpLVq79DVZXYvfulrMuyNsqBYNYKnHrqLYwbtwaA2bOHsWzZNe7+wgrOgWDWSnTqNIjKyuCkk77O66/fQ3V1CTt2PJt1WdaGOBDMWpkhQ77M+PGbAJg3bxQLF15MRE3GVVlb4EAwa4XKynpTWRmceuqP2bLlSaqrS9myZXrWZVkrV6jfVL5E0guSVkj6Yj3Tr5W0QdLCdPtoIZZr1t4NHDiFiRN3UlLSlYULJzFnzgh3lmdHLe9AkFQK3ApcCowA3i9pRD1NH4iIc9LtjnyXa2Y5paVdmTRpJ2ee+RC7di1j2rQyNm58JOuyrBUqxBbCGGBFRKyKiL3A/cDkAszXzI5A377vZtKkvXTpcgqLF1/OjBm9OHBgd9ZlWStSiEAYBKyudX9NGlfX30l6VtIvJZ1Y34wkTZE0T9K8DRs2FKA0s/alpKSMCy5YzqhRf2D//i1Mn96Vdet+knVZ1kq01EHlR4ChEXE28ARwd32NIuL2iCiPiPK+ffu2UGlmbU+vXm+jouIAPXtO4oUXrqOqSuzbtyXrsqzIFSIQ1gK1v/GfkMb9WURsiog96e4dwHkFWK6ZHYZUwujR1Zx33gIAnnqqF6+88p2Mq7JiVohAmAsMl3SSpI7AVcDU2g0kDah193JgWQGWa2ZNcMwxo6moqKFfv6tYteofqaoSe/asy7osK0J5B0JE7Ac+BfyW3Af9LyJiiaSvSbo8Nfu0pCWSFgGfBq7Nd7lm1nSSGDHiPsaMWQ7AzJkDWbHipoyrsmKjYu0Ppby8PObNm5d1GW3CO98Jjz4KRfpSWwaWL/8sa9f+EIAxY5bTtespGVdkhSJpfkSUH81jfaWyWTs0fPgPGDfuVQDmzBnO0qXvd2d55kAwa686dRpAZWUwbNi3Wb/+fqqrS9i+/Zmsy7IMORDM2rnBg7/A+PFvADB//rk880yFO8trpxwIZkZZ2bFUVgannXYXW7dOo7q6lDfeqMq6LGthDgQz+7MBAz7CxIm7KC3tyaJFb2P27OHU1OzLuixrIQ4EMztEaWkXJk7cwsiRv2H37hVMm9aRDRsezrosawEOBDOrV58+lzNp0j66dj2DJUvezbRp3ThwYFfWZVkzciCYWYNKSjowZsxSzjlnGjU1u5g+vRuvvvpfWZdlzcSBYGaNOvbYiVRUHKBXr4t58cUpqbO8N7IuywrMgWBmTSKVMGrUE5SXLwLgqad68/LL38i4KiskB4KZHZHu3c+moqKG44+/mpde+nLqLG9t4w+0oudAMLMjJokzzriHCy5YBcDMmSewfPnfZ1yV5cuBYGZHrUuXk6isDE488R9Yu/YWqqrErl0vZl2WHSUHgpnl7eSTv8OFF74GwJw5p7F48ZXuLK8VciCYWUF07Hg8lZXBySd/j40bH6K6uoRt29yFfWviQDCzgjrxxM8xYcJWABYsOJ8FC8YdvrO8mTNh8+YWqs4Ox4FgZgXXoUMPKiuD00+/h23bZqXO8p6sv/Hb3gbDhsGDD7ZskfYWDgQzazb9+1/NxIm7KSvry6JFFzNz5lBqava+teHWrXDttXDJJbDOv/eclYIEgqRLJL0gaYWkL9YzvZOkB9L02ZKGFmK5Zlb8Sks7M378es4667/Zs+dlpk3rxPr19WwN7NoFTz4Jp54Kd97p33zNQN6BIKkUuBW4FBgBvF/SiDrNrgfeiIhTgB8A3853uWbWuhx33GVMmrSPbt1GsXTpe6muLmP//h2HNtq/H3bsgBtvhAkT4KWXsim2nepQgHmMAVZExCoASfcDk4GltdpMBr6Shn8J3CJJ4fPSWtYrr2RdgbVzJcD5x09l27Z5LFnyd8x96BjG7heq23DnTpg9G0aOhK9/HT79aSgtzaDi9qUQgTAIWF3r/hrggobaRMR+SVuB44CNtRtJmgJMARg8eHABSjOAD120jh6PPglDrs66FDMAegDj/nyvge+FBw7kdiN97nNw8cVw1lktU1w7VohAKJiIuB24HaC8vNxbDwXyvs8M4H2f+RDwoaxLMTvEgQNvUtL1GLR3/1snduoEXbvCXXc5DFpIIQ4qrwVOrHX/hDSu3jaSOgA9gU0FWLaZtWKlpZ3JHYaso2tXuPJKWLUKrrii5QtrpwoRCHOB4ZJOktQRuAqYWqfNVOCaNHwl8AcfPzCzt+jcGY4/HqZOhZ//HI49NuuK2pW8AyEi9gOfAn4LLAN+ERFLJH1N0uWp2Z3AcZJWAJ8D3nJqqpm1c126wHXXwcqVcNFFWVfTLhXkGEJEPAY8VmfczbWG3wTeU4hlmVkbU1oKQ4bA/ffD2LFZV9OuFdVBZTNrh5YsgQEDcgeRLVMOBDPL1tChWVdgifsyMjMzwIFgZmaJA8HMzAAHgpmZJQ4EMzMDHAhmZpY4EMzMDHAgmJlZ4kAwMzPAgWBmZokDwczMAAeCmZklDgQzMwMcCGZmljgQzMwMcCCYmVmSVyBI6i3pCUnL099eDbQ7IGlhuk3NZ5lmZtY88t1C+CLwZEQMB55M9+uzOyLOSbfL81ymmZk1g3wDYTJwdxq+G7giz/mZmVlG8g2E4yNiXRp+DTi+gXadJc2TNEuSQ8PMrAh1aKyBpN8D/euZ9OXadyIiJEUDsxkSEWslDQP+IOm5iFhZz7KmAFMABg8e3GjxZmZWOI0GQkRc3NA0Sa9LGhAR6yQNANY3MI+16e8qSVXAaOAtgRARtwO3A5SXlzcULmZm1gzy3WU0FbgmDV8D/KZuA0m9JHVKw32A8cDSPJdrZmYFlm8gfAt4u6TlwMXpPpLKJd2R2pwBzJO0CPgj8K2IcCCYmRWZRncZHU5EbAIuqmf8POCjafhp4Kx8lmNmZs3PVyqbmRngQDAzs8SBYGZmgAPBzMwSB4KZmQEOBDMzSxwIZmYGOBDMzCxxIJiZGeBAMDOzxIFgZmaAA8HMzBIHgpmZAQ4EMzNLHAhmZgY4EMzMLHEgmJkZ4EAwM7PEgWBmZkCegSDpPZKWSKqRVH6YdpdIekHSCklfzGeZZmbWPPLdQlgMvBuY1lADSaXArcClwAjg/ZJG5LlcMzMrsA75PDgilgFIOlyzMcCKiFiV2t4PTAaW5rNsMzMrrLwCoYkGAatr3V8DXFBfQ0lTgCnp7h5Ji5u5tkLoA2zMuogmcJ2F5ToLqzXU2RpqBDjtaB/YaCBI+j3Qv55JX46I3xztgusTEbcDt6flzouIBo9LFAvXWVius7BcZ+G0hhohV+fRPrbRQIiIi4925sla4MRa909I48zMrIi0xGmnc4HhkspQO40AAARwSURBVE6S1BG4CpjaAss1M7MjkO9pp++StAYYB/y3pN+m8QMlPQYQEfuBTwG/BZYBv4iIJU2Y/e351NaCXGdhuc7Ccp2F0xpqhDzqVEQUshAzM2ulfKWymZkBDgQzM0uKJhBaSzcYknpLekLS8vS3VwPtDkhamG4tdhC9sfUjqZOkB9L02ZKGtlRtdeporM5rJW2otQ4/mkGNd0la39D1MMr5UXoOz0o6t6VrTHU0VmelpK211uXNGdR4oqQ/Slqa3uc31tMm8/XZxDqLYX12ljRH0qJU51fraXPk7/WIKIobcAa5CyqqgPIG2pQCK4FhQEdgETCihev8DvDFNPxF4NsNtNuRwTpsdP0AnwBuS8NXAQ8UaZ3XAre0dG11apgEnAssbmD6ZcDjgICxwOwirbMSeDTjdTkAODcNHwO8WM9rnvn6bGKdxbA+BXRPw2XAbGBsnTZH/F4vmi2EiFgWES800uzP3WBExF7gYDcYLWkycHcavhu4ooWXfzhNWT+16/8lcJEa6XukGRTD69ioiJgGbD5Mk8nAPZEzCzhW0oCWqe4vmlBn5iJiXUQsSMPbyZ1xOKhOs8zXZxPrzFxaRzvS3bJ0q3uG0BG/14smEJqovm4wWvrFOj4i1qXh14DjG2jXWdI8SbMktVRoNGX9/LlN5E4J3goc1yLV1VND0tDr+Hdp18EvJZ1Yz/SsFcP/Y1ONS7sXHpd0ZpaFpF0Xo8l9q62tqNbnYeqEIlifkkolLQTWA09ERIPrs6nv9Zboy+jPWrIbjHwcrs7adyIiJDV03u6QiFgraRjwB0nPRcTKQtfahj0C3BcReyR9jNw3nb/KuKbWagG5/8cdki4Dfg0Mz6IQSd2Bh4DPRMS2LGpoikbqLIr1GREHgHMkHQs8LGlkROTV/1uLBkK0km4wDlenpNclDYiIdWlzdn0D81ib/q6SVEXum0ZzB0JT1s/BNmskdQB6Apuaua66Gq0zImrXdAe5YzfFplV0y1L7Ay0iHpP0H5L6RESLdtQmqYzch+y9EfGrepoUxfpsrM5iWZ+1atgi6Y/AJeR+kuCgI36vt7ZdRsXQDcZU4Jo0fA3wli0bSb0kdUrDfYDxtEx3301ZP7XrvxL4Q6SjTi2o0Trr7Du+nNy+3GIzFfhwOjtmLLC11u7EoiGp/8F9x5LGkHvft+iXgLT8O4FlEfH9Bpplvj6bUmeRrM++acsASV2AtwPP12l25O/1LI+U1zki/i5y+wz3AK8Dv03jBwKP1Wp3Gbkj/yvJ7Wpq6TqPA54ElgO/B3qn8eXAHWn4QuA5cmfPPAdc34L1vWX9AF8DLk/DnYEHgRXAHGBYRq93Y3V+E1iS1uEfgdMzqPE+YB2wL/1vXg/cANyQpovcjz+tTK9zvWfHFUGdn6q1LmcBF2ZQ4wRyBz2fBRam22XFtj6bWGcxrM+zgWdSnYuBm9P4vN7r7rrCzMyA1rfLyMzMmokDwczMAAeCmZklDgQzMwMcCGZmljgQzMwMcCCYmVny/wE59gUbTRc+kwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc4S8nAJrTXT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We know that the length of the vectors for point a and b can be calculated by |$a_{1} - b_{1}$| and |$a_{2} - b_{2}$|\n",
        "\n",
        "So it follows that,\n",
        "\n",
        "$c^2 = (a_{1}-b_{1})^2 + (a_{2}-b_{2})^2 $\n",
        "\n",
        "$c = \\sqrt{(a_{1}-b_{1})^2 + (a_{2}-b_{2})^2} $\n",
        "\n",
        "This is the basic formula for Euclidean Distance for 2-D datapoints.\n",
        "\n",
        "However, this can be expanded to 3-D and beyond leaving us with the finalized formula of Euclidean Distance we saw above.\n",
        "\n",
        "$c = \\sqrt{(a_{1}-b_{1})^2 + (a_{2}-b_{2})^2 + (a_{3}-b_{3})^2 + ... + (a_{n}-b_{n})^2} $\n",
        "\n",
        "More succintly written as,\n",
        "\n",
        "$ \\sum_{i=1}^n (a_{i}-b_{i})^2 $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le3fSJIiEXLC",
        "colab_type": "text"
      },
      "source": [
        "**Given our understanding of the mathmatics behind calculating the Euclidean distance, how can we write that calculation in python?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAOdy--WhmJB",
        "colab_type": "text"
      },
      "source": [
        "When working with datasets, each row is a datapoint. Each column represents another dimension of the datapoint (but that leads us off track into the subject of dimensionality and furthermore the curse of dimensionality). If you don't know what I am talking about, it is a crucial concept to understand when building your own machine learning models. For the purposes of this article, I will leave further research of that topic to you.\n",
        "\n",
        "To calculate the Euclidean distance between two points we can use the following function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKZc5AmShl5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper method to calculate the square root\n",
        "def sq_rt(x):\n",
        "  return x** 0.5\n",
        "\n",
        "# Calculate the Euclidean distance between two vectors (datapoints)\n",
        "def euclidean_distance(row_1, row_2):     # For datapoint row 1 and datapoint row 2\n",
        "  # Save a distance variable to save sum of calculations to\n",
        "  distance = 0.0\n",
        "  # Itereate through each column of the row\n",
        "  # Except for the last column which is where the target varibale is stored\n",
        "  for i in range(len(row_1)-1):\n",
        "    # Calculate the length vectors for each dimension and sum them\n",
        "    distance += (row_1[i]- row_2[i])**2\n",
        "  # Return the square root of the sum of the distances\n",
        "  return sq_rt(distance)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mRkmjdBEXOI",
        "colab_type": "text"
      },
      "source": [
        "The function above assumes that the output target is the last column of the datapoint and is therefore not included in the distance calculations. In our final KNN class we will have a fit method that saves the X values and the target separately.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC3OurimEXIc",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Get Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lCsdKx6sPpm",
        "colab_type": "text"
      },
      "source": [
        "Now that we know how to calculate the distance betweeen two datapoints, we can find the k nearest neighbors (closest instances in the training data) to our new datapoint. \n",
        "\n",
        "First we can use the above function to calculate the distances between our new observation and each datapoint in our training set. Once calculated, we can sort these distances and return the instances with the smallest calculated distances.\n",
        "\n",
        "The below function get_KNN() will implement this idea in python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRtDeSs8hlx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return the k nearest neighbors to the new observation\n",
        "\n",
        "# Input is the training data, test observation, and the number of neighbors (k) to return\n",
        "def get_KNN(train, test_row, k):\n",
        "  # Save the rows and calculated distances in a tuple\n",
        "  distances = list()\n",
        "\n",
        "  # Iterate through each row in the training data\n",
        "  # Use the euclidean distance function to calculate the distances between the train row and the new observation\n",
        "  for train_row in train:\n",
        "    euc_dist = euclidean_distance(test_row, train_row)\n",
        "    # Save the row and calculated distance\n",
        "    distances.append((train_row, euc_dist))\n",
        "\n",
        "  # Sort by using the calculated distances (the second item in the tuple)\n",
        "  distances.sort(key= lambda tup: tup[1])\n",
        "  \n",
        "  # Populate a list with k nearest neighbors\n",
        "  n_neighbors = list()\n",
        "\n",
        "  # Get the nearest k neighbors by returning the k first instances in the sorted distances list\n",
        "  # Return just the first value in the tuple (the row information)\n",
        "  for i in range(k):\n",
        "    n_neighbors.append(distances[i][0])\n",
        "  \n",
        "  # Return the list of nearest neighbors\n",
        "  return n_neighbors"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yokjPz5osPw1",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Make Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVJJquonsPuV",
        "colab_type": "text"
      },
      "source": [
        "We have used our knowledge of Euclidean Distance to find the k nearest neighbors to our test datapoint. Now we can make predictions, the whole point of the model.\n",
        "\n",
        "We have the most similar instances from the dataset to our test observation. Intuatively, by looking at the target outputs of our nearest neighbors, we should be able to predict an output for our test case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zhOzuqaNMRl",
        "colab_type": "text"
      },
      "source": [
        "**Classification**:\n",
        "For a classification problem, that is as simple as counting up the instances of each output across the k nearest neighbors. Our prediction for our test datapoint will be whichever output occured most frequnetly in the nearest neighbors. \n",
        "\n",
        "The function below utilizes the output from the get_KNN() function to implement the idea of classification prediction in python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHB7RpmksQgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a classification prediction with k nearest neighbors\n",
        "def predict_classification(train, test_row, k):\n",
        "  # Find the nearest neighbors\n",
        "  n_neighbors = get_KNN(train, test_row, k)\n",
        "\n",
        "  # Populate a list with the target output (the last column) from each KNN row\n",
        "  output_values = [row[-1] for row in n_neighbors]\n",
        "\n",
        "  # Make prediction by counting each occurance of output values\n",
        "  # Return the output value that occurs the most frequently\n",
        "  prediction = max(set(output_values), key= output_values.count)\n",
        "  # Return the prediction\n",
        "  return prediction"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMMlJIUmNQtj",
        "colab_type": "text"
      },
      "source": [
        "**Regression**\n",
        "For a regression problem, we use the same logic of looking at the output values of the K nearest neighbors. Instead of returning the most common occurance, we will return the mean value of the output values as the regression prediction.\n",
        "The function below utilizes the output from the get_KNN() function to make a regression prediction in python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5YdHrz2N2Fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a regression prediction with k nearest neighbors\n",
        "def predict_regression(train, test_row, k):\n",
        "  # Find the nearest neighbors\n",
        "  n_neighbors = get_KNN(train, test_row, k)\n",
        "\n",
        "  # Populate a list with the target output (the last column) from each KNN row\n",
        "  output_values = [row[-1] for row in n_neighbors]\n",
        "\n",
        "  # Make prediction by calculating the mean of the output values from the nearest neighbors\n",
        "  prediction = sum(output_values) / len(output_values)\n",
        "  # Return the prediction\n",
        "  return prediction"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVGsWduqQ8PL",
        "colab_type": "text"
      },
      "source": [
        "The two prediction functions created above are for making a prediction for one new data point. That was primarily for ease of understanding. Generally, we are not looking for a single prediction, but a prediction for each point in a large dataset. To adapt the above functions to handle multiple predictions, just iterate through your new dataset, calling the predict function on each point\n",
        "\n",
        "The code below will accomplish that for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKX8Umj1RjOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create predictions for multiple new datapoints, classification\n",
        "\n",
        "def multiple_classifications(train, test, k):\n",
        "  # Create a list to hold all of the predictions\n",
        "\tpredictions = list()\n",
        " \n",
        "  # For each row in the test data, call the predict function\n",
        "  for row in test:\n",
        "\t\tpredicted_output = predict_classification(train, row, k)\n",
        "\t\tpredictions.append(predicted_output)\n",
        "  \n",
        "  # Return the populated list of predictions\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq8p7f4uTCcy",
        "colab_type": "text"
      },
      "source": [
        "The above can be similarly modified to handle regression predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaiwyBkhOtVF",
        "colab_type": "text"
      },
      "source": [
        "## Put the pieces together in a KNN class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdmQEjNnOw6Y",
        "colab_type": "text"
      },
      "source": [
        "Now that we have all of the pieces, we can wrap them all in a K Nearest Neighbor class. All of the functions defined above will be methods that you can call on the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjSZouHBPk6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# K Nearest Neighbors Class\n",
        "'''\n",
        "Class is initialized by setting k (number of nearest neighbors you want to look at)\n",
        "Then call the .fit() method to save the X_train matrix and y_train vector\n",
        "The method .predict_classification() will return classification predictions given X_test\n",
        "The method .predict_regression() will return regression predictions given X_test\n",
        "'''\n",
        "\n",
        "class KNN():\n",
        "  def __init__(self, k=3):\n",
        "    # k is number of nearest neighbors to return, default is 3\n",
        "    self.k = k\n",
        "\n",
        "\n",
        "  def sq_rt(self, x):\n",
        "    '''\n",
        "    Helper method to return the square root.\n",
        "    To be used in Euclidean Distance calculations.\n",
        "    '''    \n",
        "    return x**0.5\n",
        "\n",
        "  def euclidean_distance(self, row_1, row_2):\n",
        "    '''\n",
        "    Helper method to calculate the Euclidean Distance between two points, (row_1 and row_2).\n",
        "    To be used in get_KNN to calculate the closest training points to the test data.\n",
        "    '''\n",
        "    # Save a distance variable to save sum of calculations to\n",
        "    distance = 0.0\n",
        "    # Itereate through each column of the row\n",
        "    # Except for the last column which is where the target varibale is stored\n",
        "    for i in range(len(row_1)-1):\n",
        "      # Calculate the length vectors for each dimension and sum them\n",
        "      distance += (row_1[i]- row_2[i])**2\n",
        "    # Return the square root of the sum of the distances\n",
        "    return self.sq_rt(distance)\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    '''\n",
        "    Fits the training data to the model.\n",
        "    KNN does this by simply saving the training data in memory.\n",
        "    '''\n",
        "    self.X_train = X_train\n",
        "    self.y_train = y_train\n",
        "\n",
        "  def get_KNN(self, test_row):\n",
        "    '''\n",
        "    Helper method for prediction methods.\n",
        "    Will take in one test row and calculate the k nearest neighbors.\n",
        "    Returns a list of nearest neighbors.\n",
        "    '''\n",
        "\n",
        "    # Save the rows and calculated distances in a tuple\n",
        "    distances = list()\n",
        "\n",
        "    # Iterate through each row in the training data\n",
        "    for i in range(len(self.X_train)):\n",
        "      # Use the euclidean distance function to calculate the distances between the train row and the new observation\n",
        "      euc_dist = euclidean_distance(test_row, self.X_train[i])\n",
        "      # Save the index (to later recall the output value), the row data and the calculated distance\n",
        "      distances.append((i, self.X_train[i], euc_dist))\n",
        "\n",
        "\n",
        "    # Sort by using the calculated distances (the third item in the tuple)\n",
        "    distances.sort(key= lambda tup: tup[2])\n",
        "    \n",
        "    # Populate a list with k nearest neighbors\n",
        "    n_neighbors = list()\n",
        "\n",
        "    # Get the nearest k neighbors by returning the k first instances in the sorted distances list\n",
        "    # Return just the first value in the tuple (the row information)\n",
        "    for i in range(self.k):\n",
        "      n_neighbors.append(distances[i][:2])\n",
        "    \n",
        "    # Return the list of nearest neighbors\n",
        "    # Don't need to save it to self because we are just using it to populate a list\n",
        "    # Used for the prediction method.\n",
        "    # The prediction method will save the important information\n",
        "    return n_neighbors\n",
        "\n",
        "  def helper_predict_classification(self, test_row):\n",
        "    '''\n",
        "    Method returns a classification prediction for a single given test datapoint.\n",
        "    This method will be utilized in the predict_classification method which will be \n",
        "    capable of making predictions for a large X_test dataset.\n",
        "    '''\n",
        "\n",
        "    # Find the nearest neighbors\n",
        "    n_neighbors = self.get_KNN(test_row)\n",
        "\n",
        "    # Use the index values of the nearest neighbors to recall their target outputs\n",
        "\n",
        "    # Store the index values of the n_neighbors\n",
        "    train_output = [n_neighbors[i][0] for i in range(self.k)]    \n",
        "\n",
        "    # Use the index values from the n_neighbors to return their associated outputs from y_train\n",
        "    output_values = [self.y_train[value] for value in train_output]\n",
        "\n",
        "    # Make a prediction by counting each occurance of output values\n",
        "    # Return the output value that occurs the most frequently\n",
        "    prediction = max(set(output_values), key= output_values.count)\n",
        "    # Return the prediction\n",
        "    return prediction\n",
        "\n",
        "  def predict_classification(self, X_test):\n",
        "    '''\n",
        "    Method utilizes the helper_predict_classification to return predictions for \n",
        "    multiple test rows stored in X_test.\n",
        "    '''\n",
        "    self.X_test = X_test\n",
        "\n",
        "    # Create a list to hold all of the predictions\n",
        "    predictions = []\n",
        "\n",
        "    # For each row in X_test, call the predict_classification helper method\n",
        "    for test_row in self.X_test:\n",
        "      predicted_output = self.helper_predict_classification(test_row)\n",
        "\n",
        "      # Save the prediction for each row in X_test\n",
        "      predictions.append(predicted_output)\n",
        "\n",
        "    # Return the list of predictions for each datapoint in X_test\n",
        "    return predictions\n",
        "\n",
        "  def helper_predict_regression(self, test_row):\n",
        "    '''\n",
        "    Method returns a regression prediction for a single given test datapoint.\n",
        "    This method will be utilized in the predict_classification method which will be \n",
        "    capable of making predictions for a large X_test dataset.\n",
        "    '''\n",
        "\n",
        "    # Find the nearest neighbors\n",
        "    n_neighbors = self.get_KNN(test_row)\n",
        "\n",
        "    # Use the index values of the nearest neighbors to recall their target outputs\n",
        "\n",
        "    # Store the index values of the n_neighbors\n",
        "    train_output = [n_neighbors[i][0] for i in range(self.k)]    \n",
        "\n",
        "    # Use the index values from the n_neighbors to return their associated outputs from y_train\n",
        "    output_values = [self.y_train[value] for value in train_output]\n",
        "\n",
        "    # Make prediction by calculating the mean of the output values from the nearest neighbors\n",
        "    prediction = sum(output_values) / len(output_values)\n",
        "\n",
        "    # Return the prediction\n",
        "    return prediction\n",
        "\n",
        "  def predict_regression(self, X_test):\n",
        "    '''\n",
        "    Method utilizes the helper_predict_classification to return predictions for \n",
        "    multiple test rows stored in X_test.\n",
        "    '''\n",
        "    self.X_test = X_test\n",
        "\n",
        "    # Create a list to hold all of the predictions\n",
        "    predictions = []\n",
        "\n",
        "    # For each row in X_test, call the predict_classification helper method\n",
        "    for test_row in self.X_test:\n",
        "      predicted_output = self.helper_predict_regression(test_row)\n",
        "\n",
        "      # Save the prediction for each row in X_test\n",
        "      predictions.append(predicted_output)\n",
        "\n",
        "    # Return the list of predictions for each datapoint in X_test\n",
        "    return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv3QvxV9Ow--",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8u-GV_FOw9R",
        "colab_type": "text"
      },
      "source": [
        "## Comparing Scikit-Learn's KNN with our own KNN algorithm\n",
        "Let's see how our algorithm compares to Scikit-Learn's KNN implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP8bBL25j2Iy",
        "colab_type": "text"
      },
      "source": [
        "### Titanic Dataset\n",
        "We will work with a very common and easily accessable dataset to make it easier for readers to follow along.\n",
        "\n",
        "The Titanic dataset is one of the most popular datasets for begining to learn classification models. The data is already cleaned and has a clear classification target of survived or did not survive. After only a few basic pre-processing steps, we will have a perfect dataset for our KNN model.\n",
        "\n",
        "We will only work with classification. After following along with this article, try to implement a regression problem with the KNN class we created.\n",
        "\n",
        "*Download your own copy of the Titanic dataset and follow along.*  https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vAdU9YdThAR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXZJ5J6UTg96",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je7uqh3STg6l",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNuyfPvPTg3D",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHkE4Hx2Tg0l",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbazOqDlmDxe",
        "colab_type": "text"
      },
      "source": [
        "### Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAoUTbNhmIhv",
        "colab_type": "text"
      },
      "source": [
        "*Jason Brownlee, 'Develop k-Nearest Neighbors in Python From Scratch', Machine Learning Mastery. https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/*"
      ]
    }
  ]
}